hydra:
  job:
    chdir: false
defaults:
  - model_config: hf-gen_a      # the model config of the LM inferencer, can be `hf-gen_a` or `api-gen-a` 

model_name: ???                # the model name of the LM inferencer
task_name: ??? 
output_file: ???                # predictions will be saved to `output_file`

batch_size: 1              # the batch_size of the model when using `hf-gen_a` model_config; for api models, the batch size is decided based on the number of openai keys.

# parameters needed to initialize the inference dataset reader
dataset_reader:
  _target_: src.dataset_readers.inference_dsr.InferenceDatasetReader
  dataset_path: null          # one of `dataset_path` and `dataset_split` must be set  
  dataset_split: null
  task_name: ${task_name}
  model_name: ${model_name}
  n_tokens: ???                # maximum number of tokens as prompt
  field: gen_a                  # this field will be used to construct prompt
  field_pd: qa1a2
  is_mixed: 1
  len_qa: 5
  len_pd: 5
  ds_size: null                # number of instances used for the dataset, 'null' refers to 'all'
  index_reader: ${index_reader}

# parameters needed to initialize the index reader
index_reader:
  _target_: src.dataset_readers.index_dsr.IndexDatasetReader
  task_name: ${task_name}
  model_name: ${model_name}
  field: qa                   # the field used for in-context examples, `qa` refers to the whole input-output pairs
  field_pd: qa1a2
  is_mixed: 1
  len_qa: 5
  len_pd: 5
  dataset_path: null          # one of `dataset_path` and `dataset_split` must be set
  dataset_split: null
  ds_size: null

# # to enable fp16 method in inferencer.py
# model_config: 
#   model_type: hf               # Use HuggingFace models
#   model:
#     _target_: transformers.AutoModelForCausalLM.from_pretrained
#     pretrained_model_name_or_path: ${model_name}
#     torch_dtype: float16        #  Enable FP16 precision
#   tokenizer:
#     _target_: transformers.AutoTokenizer.from_pretrained
#     pretrained_model_name_or_path: ${model_name}
#     use_fast: true
#   generation_kwargs:
#     fp16: true                  #  Ensure FP16 is enabled in generation parameters
#     do_sample: False
#     max_length: 1024

# #  Enable mixed precision in accelerator settings
# device_config:
#   mixed_precision: fp16         #  Enable FP16 mixed precision